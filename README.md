![reversal](https://capsule-render.vercel.app/api?type=waving&text=Compass%20UOL&fontSize=50&animation=fadeIn&height=210&fontAlignY=30&desc=Repositório%20do%20Programa%20de%20Bolsas&descSize=25&descAlignY=50&color=gradient&customColorList=27)
# Sobre mim
Olá! Meu nome é Ísis Yasmim, tenho 23 anos e venho de Manaus mas atualmente resido na cidade de Medianeira, no oeste do Paraná. 

Faço o curso de __ciência da computação__ na UTFPR, e estou no quarto período. Minha experiência na área
consiste em um projeto de conclusão de curso do ensino médio técnico que desenvolvi em Java e
utilizando o MySQL, e também um projeto de iniciação científica apresentado no _SEI/SICITE_ de 2022,
desenvolvido em Flutter.

<p align = "center">
<a href="https://github.com/anuraghazra/github-readme-stats">
  <img src="https://github-readme-stats.vercel.app/api/top-langs/?username=Isisyasmim&size_weight=0.5&count_weight=1&layout=donut&theme=tokyonight" />
</a>
</p>




<img align = "right" width="125" height="125" hspace="3" src="https://i.imgur.com/9KmYrKv.jpg">

Tenho vários hobbies aleatórios, como jogos, animes, costura e cubos mágicos; de vez em quando faço cosplay. Sou apaixonada pela cor rosa e por gatinhos!
 
Ao lado mostro a minha coleção de cubos mágicos!

# Conteúdo sendo aprendido
<details>
<summary>Sprint 1</summary>

## Sprint 1
### Git/Github
* Principais conceitos do Git e Github: Compreendendo a estrutura de um repositório Git, como branches, commits, tags e HEAD, além dos comandos básicos como init, clone, add, push e pull. Aprendendo a estruturação correta de um repositório simulando o uso real numa empresa, com branching e mesclagem (merging), e também a reverter alterações e desfazer commits.
### Linux
* Conceitos de Linux para desenvolvedores: Aprendendo comandos básicos de terminal, como cd, ls, clear, entre outros. Entendendo como estruturar tanto diretórios quanto arquivos e como os criar, alterar, remover, copiar, mover e achar, e também a como utilizar os editores de texto nano e vim. Gerenciando pacotes e aplicativos, incluindo a remoção, upgrade e update dos mesmos, e também gerenciando usuários.
</details>

<details>
<summary>Sprint 2</summary>

## Sprint 2
### SQL
* SQL para análise de dados: Vendo comandos básicos, como a recuperar dados de uma tabela, filtrar registros, ordenar resultados e limitar o número de linhas, além de como agrupar dados por categorias ou critérios específicos. Aprendendo também a realizar cálculos e resumos estatísticos nos dados, como soma, contagem, média, máximo e mínimo.
### Big Data
* Big Data Fundamentos: Foi apresentado uma introdução abrangente aos principais conceitos e tecnologias relacionadas ao processamento e análise de grandes volumes de dados. Durante o curso, foi ensinado sobre armazenamentos como bancos de dados relacionais, bancos de dados NoSQL, sistemas de arquivos distribuídos e sistemas de armazenamento em nuvem, como também o curso abrange os fundamentos da computação em nuvem e seu papel no contexto do Big Data. Também foi ensinado os conceitos de MLOps e DataOps, Dados como Serviço (Data-as-a-Service) e ETL (Extração, Transformação e Carregamento).
</details>

<details>
<summary>Sprint 3</summary>

## Sprint 3
### Python
* Python do Básico ao Avançado: Houve uma introdução à programação com conceitos básicos, estrutura de um programa Python, tipos de dados e controle de fluxo, assim como apresentação de conceitos de listas, tuplas, conjuntos e dicionários. Foi ensinado a manipulação de arquivos: leitura e escrita de arquivos, assim como a criação e uso de funções. Também foi ensinado conceitos básicos e conceitos avançados de Programação Orientada a Objetos, como herança, polimorfismo, encapsulamento e entre outros.
</details>

<details>
<summary>Sprint 4</summary>

## Sprint 4
### Programação Funcional com Python
* Programação Funcional com Python: Foi ensinado conceitos de programação funcional como funções de primeira classe, o uso de lambda, imutabilidade de dados, o uso das funções map, filter e reduce e funções de alta ordem.

### Docker
* Docker para Desenvolvedores: Nesse curso é apresentado tanto ideias básicas como avançadas do uso de Docker. É ensinado a criar, deletar, buildar e utilizar imagens abrangendo não só uma linguagem, mas várias. Foi visto conceito de containers, como criar, deletar, mostrar listas, nomear e usar de forma iterativa. Além disso, houve uma ampla compreensão dos conceitos de volume em Docker, Networks, YAML e Kubernetes.

### Estatística com Python
* Estatística Descritiva com Python: É visto e aprendido como utilizar Python para diversos usos básicos de estatística, assim como é visto esses mesmos conceitos necessários para analisar dados. Em Python, existem várias bibliotecas poderosas, como NumPy, pandas e matplotlib, que facilitam a realização de análises estatísticas descritivas. São utilizadas para apresentar a média, moda e mediana de dados, assim como é mostrado a forma de calcular variância, amplitude, desvio padrão e CV. Há também formas de plotar diversos gráficos utilizando as bibliotecas apresentadas.
</details>

<details>
<summary>Sprint 5</summary>

## Sprint 5
### AWS Skill Builder
* AWS Partner Sales Accreditation: No curso foi apresentado os principais serviços da AWS, e também foi demonstrado a metodologia de venda desses serviços, assim como identificar oportunidades, construir propostas e entender as necessidades do cliente. É orientado sobre como entender a estrutura de preços da AWS e como apresentar aos clientes as opções de licenciamento mais adequadas. A badge recebida do curso pode ser acessada por [esse link.](https://www.credly.com/badges/403ea6f9-335e-40fa-adf4-658079015c42/public_url)

* AWS Partner - Accreditation (Technical): O curso explora os serviços fundamentais da AWS, como Amazon EC2 (Elastic Compute Cloud), Amazon S3 (Simple Storage Service), Amazon RDS (Relational Database Service), entre outros. Foi aprendido a projetar e implementar arquiteturas escaláveis e resilientes na AWS, usando práticas recomendadas e padrões arquiteturais. A badge recebida do curso pode ser acessada por [esse link.](https://www.credly.com/badges/ab3439f7-3066-4245-8781-bc4464871fe9/public_url)

* AWS Partner Cloud Economics Accreditation: O curso é uma iniciativa da AWS desenvolvida para capacitar profissionais de vendas e negócios de parceiros a compreender e comunicar efetivamente os princípios da economia em nuvem da AWS. O objetivo do curso é ensinar como a adoção da nuvem pode impactar os custos e benefícios dos clientes, e abrange uma variedade de tópicos relacionados à economia em nuvem. A badge recebida do curso por ser acessada por [esse link.](https://www.credly.com/badges/db9714ab-78ab-42b0-9582-b519f01aa673/public_url)

* AWS Exam Prep:  O curso tem como objetivo ensinar a se preparar adequadamente para o exame de certificação AWS Certified Cloud Practitioner, abordando os principais tópicos e competências necessárias para obter a certificação, junto com perguntas e exemplos pertinentes ao material aprendido previamente. O curso não oferece badge, então o print para comprovação de conclusão está na pasta Sprint5.

* Data & Analytics - PB - AWS 5/10: Nesse exercício, é pedido para criar uma instância EC2 com as seguintes tags: Project (use o valor Programa de Bolsas), CostCenter (use o valor Data & Analytics), Name (valor a seu interesse). Estava obtendo erro de autorização por conta da dificuldade no entendimento da criação das tags, porém com a ajuda do Patrick e do Lucas Ricieri pude alterar as tags para incluir Instâncias + Volumes e consegui concluir o exercício proposto.


### Cloud Quest - Missões Feitas
A badge recebida pelo jogo pode ser acessada por [esse link.](https://www.credly.com/badges/fbcf800d-c345-4332-a9b5-d94093799f21/public_url) Abaixo faço uma descrição do que foi feito em cada laboratório dentro do jogo:
* Cloud Computing Essentials: Implementei uma instância de Amazon S3 para hosting de um website estático.

* Cloud First Steps: Criei, a partir de uma imagem, uma segunda instância em uma Zona de Disponibilidade diferente. Tive certa dificuldade de início, mas ao pesquisar no google descobri que era apenas atribuir a uma outra subnet.

* Computing Solutions: Após parar a instância EC2, foi possível mudar o tipo dela para m4.large.

* Networking Concepts: Ativei a comunicação do VPC com a internet a partir das rotas de tabela e grupos de seguramça.

* Databases in Practice: Criei uma replica de leitura da Amazon RDS criada durante o período de prática.

* Connecting VPCs: Usando conceitos de emparelhamento apresentados, estabeleci uma conexão entre duas instâncias (Developer e Finance). Não entendi os passos de primeira, portanto revi a prática dessa quest mais uma vez.

* First NoSQL Database: Usando a tabela da Amazon DynamoDB criada durante a sessão de prática, adicionei mais um atributo a um novo item criado.

* File Systems in the Cloud: Usando Amazon EFS, criei múltiplos pontos de acesso ao banco de dados existente.

* Auto-healing and Scaling Applications: Com os conceitos de Auto Scaling disponíveis no EC2, implementei um horário fixo de desligamento dos servidores.

* Highly Available Web Applications: Usando AWS Application Load Balancer (ALB) e Auto Scaling, foi possível aumentar a disponibilidade do site em tempos de pouco uso dos servidores e alto uso dos servidores. 

* Core Security Concepts: Usei conceitos de IAM, como permissões e usuários, para criar um grupo de usuários com acesso restrito.
* Cloud Economics: Fiz uma estimativa de preço no site da Amazon para um caso específico.
</details>

<details>
<summary>Sprint 6</summary>

## Sprint 6
### AWS Partner
* Data Analytics on AWS: O curso oferece acesso a treinamento especializado, recursos técnicos e materiais educacionais que ajudam os parceiros de negócios a desenvolverem suas competências em análise de dados na AWS, com exemplo de cenários e exercícios práticos.
### AWS Skill Builder
* Data Analytics Fundamentals: Nesse curso foi aprendido um conhecimento de base sobre conceitos e práticas relacionadas à análise de dados de Big Data, como o significado dos quatro V's (Volume, Velocidade, Variedade e Veracidade) assim como os elementos atrelados à esses conceitos.

* Introduction to Amazon Kinesis Streams:  O vídeo ofertado tem o objetivo de explicar os conceitos fundamentais e o funcionamento do serviço Amazon Kinesis Streams. 

* Introduction to Amazon Kinesis Analytics: É um breve vídeo que demonstra o uso do Amazon Kinesis para análise de dados, com um simples overview.

* Introduction to Amazon Elastic MapReduce (EMR):  É uma introdução abrangente ao serviço EMR da AWS, projetado para equipar os participantes com as habilidades necessárias para processar e analisar grandes volumes de dados de maneira escalável e eficiente na plataforma AWS.

* Introduction to Amazon Athena:  O curso é uma introdução ao Amazon Athena, explicando que é um serviço de análise de dados sem servidor que permite executar consultas SQL em dados armazenados no Amazon S3, sem a necessidade de configuração de infraestrutura.

* Introduction to Amazon Quicksight: É ensinado o conceito e uso do Amazon QuickSight, destacando que é uma ferramenta de análise de dados que permite criar visualizações interativas e painéis de controle a partir de dados armazenados em diferentes fontes.

* Introduction to AWS IoT Analytics: Aprende-se o uso do AWS IoT Analytics, destacando seu papel na análise de dados provenientes de dispositivos IoT para obter insights valiosos.

* Getting Started with Amazon Redshift:  É uma visão geral do Amazon Redshift, explicando como usá-lo no armazenamento de grandes volumes de dados.

* Deep Dive into Concepts and Tools for Analyzing Streaming Data: O curso explica maneiras que são feitas as análises de queries, como ordená-las e como fazer uso desses conceitos com ferramentas da AWS.

* Best Practices for Data Warehousing with Amazon Redshift: É oferecido uma visão abrangente das melhores práticas para projetar, configurar e gerenciar eficazmente um data warehouse usando o Amazon Redshift.

* Serverless Analytics: É ensinado como projetar, implementar e gerenciar soluções de análise de dados usando arquiteturas sem servidor e serviços na nuvem, com ênfase nas melhores práticas, ferramentas e casos de uso. 

* Why Analytics for Games: Concentra-se na importância de usar análises na indústria de jogos para melhorar o desenvolvimento de jogos, a experiência do usuário e os retornos comerciais.
</details>

<details>
<summary>Sprint 7</summary>

## Sprint 7
### Hadoop
* Learn By Example - Hadoop, MapReduce for Big Data problems: Este curso fornece uma base sólida para profissionais que desejam entrar no campo do processamento de Big Data usando tecnologias Hadoop e MapReduce. Há, de início, uma introdução à fundamentos do Hadoop, sua estrutura de funcionamento do framework. Depois, é ensinado o paradigma de programação MapReduce e como ele é usado para processar dados em clusters do Hadoop. Também é explorado o HDFS (Hadoop Distributed File System) para armazenação de grandes volumes de dados e gerenciamento desses dados.

### Spark
* Formação Spark com Pyspark: O curso aborda desde a instalação e configuração do ambiente Spark até a exploração de conceitos fundamentais, como RDDs e DataFrames, além de ensinar transformações e ações para manipulação de dados. Também é ensinado a integrar o Spark com diversas fontes de dados, lidar com processamento em lote e em streaming, e explorar o ecossistema Spark com a linguagem Python.

### Tarefa: Apache Spark - Contador de Palavras
O conteúdo dessa atividade pode ser acessado por meio [deste link do Google Colab.](https://colab.research.google.com/drive/102mt2GBW_v7StN9aTHjCPnOvV4nkCWSK?usp=sharing)

### Tarefa: Desafio Parte 1 - ETL
Segue os comandos necessário para execução da imagem Docker no caminho *Sprint7/desafio_ETL/* desse repositório:
```
docker build -t desafio_etl .
```
Após isso, execute o comando para rodar o *container* com volume (as chaves de acesso serão informadas somente ao rodar o container por motivos de segurança):
```
docker run --name etl -v dados:/app -e AWS_ACCESS_KEY_ID=[insira sua chave de acesss] -e AWS_SECRET_ACCESS_KEY=[insira sua chave de acesso secreta] desafio_etl
```

</details>

<details>
<summary>Sprint 8</summary>

## Sprint 8
### Tarefa 2 - Desafio Parte 2
Primeiramente, criei um layer no AWS Lambda para utilizar todas as bibliotecas necessárias para rodar o código, incluindo Pandas, NumPy e Requests para uso da API do TMDB. O arquivo zip utilizado no layer se encontra no caminho *Sprint8/Tarefa2* deste repositório.

Após criar o layer e criar uma nova função no AWS Lambda para Python versão 3.11, realizei o teste do código, apresentado no arquivo com caminho *Sprint8/Tarefa2/analisetmdb.py* deste repositório, com objetivo de acessar o Bucket do S3 criado no exercício prévio, baixar o arquivo movies.csv armazenado nesse Bucket e utilizar os IDs filtrados de filmes armazenados no CSV com nota maior de 5 e gênero Horror para realizar pesquisas no TMDB e retornar apenas os filmes que tenham um orçamento inferior a $500.000,00. O objetivo é analisar se há relação entre nota de avaliação e orçamento do filme.

### Tarefa 3 - Geração de massa de dados
Foi gerado todos os códigos que foram solicitados pela atividade, assim como o código python para gerar um arquivo txt de nome *nomes_aleatorios.txt* para ser usado na próxima tarefa.

### Tarefa 4 - Apache Spark
A tarefa foi realizada no Google Colab com o seguinte [link para acesso](https://colab.research.google.com/drive/1AF_4lGshGE32RO6s4RUR3HeOqubw_0rY?usp=sharing), e o arquivo ipynb também pode ser acessado no caminho *Sprint8/Tarefa3/nomesaleatorios.ipynb* nesse repositório. 

Após instalação do pyspark e importação das bibliotecas necessárias, assim como inicialização do SparkSession, armazenei o conteúdo do arquivo txt gerado na tarefa anterior no *dataframe* de nome *df_nomes*. Renomeei a coluna do *dataframe* para *Nomes*, e adicionei, com valores aleatórios conforme solicitado na atividade, outras três colunas nomeadas *Escolaridade*, *Pais* e *AnoNascimento*. Em seguida, usei tanto o comando select() quanto *queries* de SQL para análise dos dados.

</details>

<details>
<summary>Sprint 9</summary>

## Sprint 9
### Tarefa 1 - Modelagem Relacional
Após fazer o planejamento da modelagem por meio do MySQL Workbench, com o arquivo e print das tabelas disponível no caminho *Sprint9/Tarefa1-Modelagem_Relacional* desse repositório, criei o script SQL e normalizei o banco de dados original pelo programa DBeaver, o script pode ser acessado no caminho *Sprint9/Tarefa1-Modelagem_Relacional/script_normalizacao.sql* desse repositório.

### Tarefa 2 - Modelagem Dimensional
Repeti o mesmo procedimento da tarefa anterior, fazendo a modelagem previamente pelo MySQL Workbench com print das tabelas podendo ser acessado pelo caminho *Sprint9/Tarefa2-Modelagem_Dimensional*, assim como o script SQL feito para criar as views.

### Tarefa 3 - Camada Trusted
Criei, manualmente, uma camada Trusted no bucket do S3, e pelo AWS Glue fiz um job com o objetivo de armazenar o JSON criado na sprint anterior primeiro em um DataFrame e, após isso, transformá-lo em um DynamicFrame para ser efetuado um tratamento de dados, removendo notas de filmes que sejam iguais à 0. Esse DynamicFrame foi então convertido em parquet, adicionado à camada Trusted no Bucket do S3 e foi atualizado no catálogo do AWS Glue como uma nova tabela.

O código feito no AWS Glue pode ser visto no caminho *Sprint9/Tarefa3-Trusted/job_trusted.py*, assim como o arquivo parquet gerado pelo job.

### Tarefa 4 - Dimensionamento Refined
Fiz o dimensionamento pelo MySQL Workbench e as tabelas foram criadas no catálogo do AWS Glue com um Crawler para cada parquet, que foi separado em pastas para cada tabela. Os prints podem ser acessados no caminho *Sprint9/Tarefa4-Modelagem_Refined*.

### Tarefa 5 - Processamento Refined
Criei um job no AWS Glue para criar um DynamicFrame a partir do parquet da camada Trusted gerado na Tarefa 3, o código funciona criando um DataFrame para cada tabela do modelo dimensional, fazendo os devidos cálculos necessários para análise. Após isso, é adicionado na camada Refined, em pastas separadas, cada parquet de cada tabela. Usei Crawlers para registrar os dados em tabelas no catálogo.

Para facilitar a testagem do job e evitar gastos desnecessários, modifiquei um código existente na internet que possibilita o uso da biblioteca do Glue no Google Colab, sem custos adicionais. O código pode ser acessado por [esse link.](https://gist.github.com/IsisYasmim/a9734ee8b3db1f8dee988c32ee5d345e)

</details>

<details>
<summary>Sprint 10</summary>

### Desafio - Parte IV
No começo da sprint, seguindo o conselho da instrutora Isabela, mudei o foco da minha análise e usei um [dataset do Kaggle](https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset?select=rotten_tomatoes_movies.csv) com lista de diretores por filme para fazer uma análise a partir de dados específicos dos diretores pela API do TMDB. O novo algoritmo para chamada da API e os arquivos json gerados estão no diretório *Sprint8*; os jobs atualizados, assim como a modelagem nova de análise, estão disponíveis no diretório da *Sprint9*.

A partir dos parquets gerados para a nova análise, criei um DashBoard no QuickSight com o objetivo de analisar o faturamento, nota média e popularidade de filmes de terror categorizados por gênero do(s) diretor(es), considerando Feminino e Masculino. A partir desses dados, foi possível visualizar uma diferença clara entre o faturamento (receita) dos filmes quando analisados por gênero, embora a nota e a popularidade não tenham uma discrepância tão acentuada. Os arquivos PDFs gerados a partir do Dashboard podem ser acessados no diretório da *Sprint10*.

</details>

# Considerações finais
A participação nesse programa de bolsas foi crucial pro meu desenvolvimento pessoal e profissional. Aprendi conteúdos técnicos que há muito tempo queria aprender e adquiri um novo fascínio por essa área, que é o processo de extração e análise de dados; além disso, a oportunidade de trabalhar/estudar em equipe e conhecer pessoas de diversas áreas do Brasil foi interessante e bastante enriquecedora. Muito obrigada!